#-- stima modello lineare semplice
mod <- lm(y~x,dati)
mod$coefficients
mod
summary(mod)
dati$residuo <- mod$residuals #-- residui del modello
dati$fitted_values <- mod$fitted.values #-- valori fittati
dati$residuo_manuale <- dati$y - dati$fitted_values
dati$peso <- dati$y
dati$altezza <- dati$x
#-- y-X*beta
y[1]-mod$fitted.values[1]
plot(dati$x,dati$y)
abline(mod,col=2,lwd=3)
nuovi_dati <- data.frame(x=6)
predict(mod,nuovi_dati)
summary(mod)
cbind(1,dati$x)
coef(mod)
mod
(cbind(1,dati$x) %*% coef(mod) )[1:10]
mod$fitted.values[1:10]
hist(mod$residuals)
plot(dati$x,mod$residuals)
n <- 200 #-- quanti dati simulare
x <- rnorm(n,5,3) #-- x
y <- 2*x + 1/9*x^2+ rnorm(n,sd=1) #-- outcome
dati <- data.frame(y,x) #-- dataset
plot(dati$x,dati$y)
dati <- data.frame(y,x) #-- dataset
plot(dati$x,dati$y)
dati$x_quad <- dati$x^2
mod_lin <- lm( y ~ x,dati)
mod_quad <- lm( y ~ x + x_quad,dati)
summary(mod_lin)
summary(mod_quad)
grid <- seq(-5,20,length.out = 2000)
plot(dati$x,dati$y,pch=19,cex=.4)
lines(grid,predict(mod_lin,data.frame(x=grid)),col=2,lwd=2)
lines(grid,predict(mod_quad,data.frame(x=grid,x_quad=grid^2)),col=4,lwd=2)
range(x)
#-- import dati
d <- read.csv("../datasets/car.test.txt",sep=" ")
#-- import dati
d <- read.csv("./datasets/car.test.txt",sep=" ")
getwd()
ls
!ls
#-- import dati
d <- read.csv("/home/pranav/Desktop/Statistical Modelling/Laboratory/datasets/car.test.txt",sep=" ")
names(d)
table(d$Type)
View(d)
View(d)
VAR_NUMERIC <- c("Price","Mileage","Weight","Disp.")
d[,4]
d[,"Reliability"]
d[,VAR_NUMERIC]
summary(d[,VAR_NUMERIC])
plot(d[,VAR_NUMERIC],cex=.8)
cor(d[,VAR_NUMERIC])
boxplot(d$Price ~ d$Type,col=2)
cor(d[,VAR_NUMERIC])
boxplot(d$Price ~ d$Type,col=2)
plot(d[,VAR_NUMERIC],cex=.8)
boxplot(d$Price ~ d$Type,col=2)
d$Disp._l <- 2+d$Disp.
mod_quad <- lm( Price ~ Disp. + I(Disp.^2),d)
mod_lin_log <- lm( log(Price) ~ Disp.,d)
summary(mod_lin_log)
plot(mod_lin,which=1)
plot(mod_lin,which=2)
plot(mod_lin,which=3)
plot(mod_lin,which=4)
plot(mod_lin,which=5)
plot(mod_lin,which=6)
anova(mod_lin)
summary(mod_lin)
summary(mod_quad)
grid <- seq(min(d$Disp.),max(d$Disp.),length.out = 40000)
plot(d$Disp.,d$Price,pch=19,cex=.7)
lines(grid,predict(mod_lin,data.frame(Disp.=grid)),col=2,lwd=2)
lines(grid,predict(mod_quad,data.frame(Disp.=grid)),col=3,lwd=2)
lines(grid,exp(predict(mod_lin_log,data.frame(Disp.=grid))),col=4,lwd=2)
hist(mod_lin$residuals)
hist(mod_quad$residuals)
shapiro.test(mod_lin$residuals)
shapiro.test(mod_quad$residuals)
plot(d$Disp.,mod_lin$residuals)
lines(lowess(d$Disp.,mod_lin$residuals)$x,lowess(d$Disp.,mod_lin$residuals)$y,col=2,lwd=3)
mod_lin <- lm( Price ~ Disp. + Type,d)
summary(mod_lin)
lines(lowess(d$Disp.,mod_lin$residuals)$x,lowess(d$Disp.,mod_lin$residuals)$y,col=2,lwd=3)
mod_lin <- lm( Price ~ Disp. + Type,d)
summary(mod_lin)
plot(d$Disp.,mod_lin$residuals)
shapiro.test(mod_quad$residuals)
shapiro.test(mod_lin$residuals)
Disp
Disp.
d
d$Disp._l
d$Disp.
d$Disp
d$Disp.
View(d)
hist(d$Price)
hist(log(d$Price))
d$Disp._l
d$Disp._l + I(d$Disp._l^2)
I(d$Disp._l)
I(d$Disp._l) + d$Disp._l
d$Disp._l + d$Disp._l
mod_lin <- lm( Price ~ Disp.,d)
mod_quad <- lm( Price ~ Disp. + I(Disp.^2),d)
mod_lin_log <- lm( log(Price) ~ Disp.,d)
summary(mod_lin_log)
plot(mod_lin,which=1)
summary(mod_lin)
summary(mod_quad)
summary(mod_lin_log)
plot(mod_lin,which=1)
plot(mod_lin,which=2)
plot(mod_quad,which=1)
plot(mod_lin_log,which=1)
plot(mod_quad,which=1)
plot(mod_lin,which=1)
plot(mod_lin,which=2)
plot(mod_quad,which=2)
plot(mod_quad,which=3)
plot(mod_quad,which=4)
plot(mod_lin,which=3)
plot(mod_quad,which=4)
anova(mod_lin)
grid <- seq(min(d$Disp.),max(d$Disp.),length.out = 40000)
plot(d$Disp.,d$Price,pch=19,cex=.7)
lines(grid,predict(mod_lin,data.frame(Disp.=grid)),col=2,lwd=2)
lines(grid,predict(mod_quad,data.frame(Disp.=grid)),col=3,lwd=2)
lines(grid,exp(predict(mod_lin_log,data.frame(Disp.=grid))),col=4,lwd=2)
hist(mod_lin$residuals)
hist(mod_quad$residuals)
shapiro.test(mod_lin$residuals)
shapiro.test(mod_quad$residuals)
plot(d$Disp.,mod_lin$residuals)
lines(lowess(d$Disp.,mod_lin$residuals)$x,lowess(d$Disp.,mod_lin$residuals)$y,col=2,lwd=3)
mod_lin <- lm( Price ~ Disp. + Type,d)
summary(mod_lin)
summary(mod_lin)
shapiro.test(rnorm(200))
shapiro.test(rnorm(2000))
shapiro.test(rnorm(200000))
shapiro.test(rnorm(50000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
shapiro.test(rnorm(5000))
hist(d$Price,freq=F)
curve( dnorm(x,mean(d$Price),sd(d$Price)), add=T,col=2,lwd=3)
hist(log(d$Price),freq=F)
curve(dnorm(x,mean(log(d$Price)),sd(log(d$Price))),add=T,col=2,lwd=3)
hist(d$Price,freq=F)
curve( dnorm(x,mean(d$Price),sd(d$Price)), add=T,col=2,lwd=3)
hist(log(d$Price),freq=F)
curve(dnorm(x,mean(log(d$Price)),sd(log(d$Price))),add=T,col=2,lwd=3)
shapiro.test(d$Price)
shapiro.test(log(d$Price))
FIND_EXTREME_OBSERVATION <- function(x,sd_factor=2){
which(  x>mean(x)+sd_factor*sd(x) |
x<mean(x)-sd_factor*sd(x)
)
}
out_y <- FIND_EXTREME_OBSERVATION(d$Price)
out_x <- FIND_EXTREME_OBSERVATION(d$Disp.)
out_x
out_y
plot(d$Disp.,d$Price,pch=19,cex=.7)
points(d$Disp.[out_y],d$Price[out_y],pch=19,cex=2,col=2)
points(d$Disp.[out_x],d$Price[out_x],pch=19,cex=2,col=3)
white.test(mod_lin)
white.test(mod_quad)
dwtest(mod_lin)
sel <- d[-c(out_x,out_y),] #elimino gli outlier
mod_lin <- lm( Price ~ Disp.,d)
mod_lin1 <- lm( Price ~ Disp.,sel)
plot(d$Disp.,d$Price)
abline(mod_lin)
abline(mod_lin1, col=2)
library(het.test)
library(olsrr)
res <- ols_step_forward(mod_lin)
res$steps
res$re
res$predictors
plot(res)
res <- ols_step_forward(mod_lin)
res$steps
res <- ols_step_forward(mod_lin)
res
res <- ols_step_forward_p(mod_lin)
res <- ols_step_forward_p(mod_lin)
mod_lin <- lm( Price ~ Disp.+ Mileage + Reliability + HP + Weight,d)
summary(mod_lin)
res <- ols_step_forward_p(mod_lin)
summary(mod_lin)
res <- ols_step_forward_p(mod_lin)
res$steps
res$re
res$predictors
plot(res)
res <- ols_step_backward_p(mod_lin)
res$removed
plot(res)
res <- ols_step_forward_p(mod_lin)
res$steps
res$re
res$predictors
plot(res)
res$predictors
res <- ols_step_backward_p(mod_lin)
res$re
res$predictors
plot(res)
res$removed
res$re
---
title: 'NORM_COL 1 - Data set: CUPS'
output:
pdf_document: default
html_document: default
word_document: default
editor_options:
chunk_output_type: console
chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## INTRODUZIONE
Il data set contiene performance di misura e caratteristiche di 209 CPUs. Le variabili sono le seguenti:
1. NAME: produttore del modello
2. SYCT: cycle time in nanosecondi
3. MMIN: minimim main memory in KB
4. MMAX: maximum main memory in KB
5. CACH: cache size in KB
6. CHMIN: minimum number of channels
7. CHMAX: maximum number of channels
8. PERF: performance della CPU comparata con il modello IBM 370/158-3
9. ESTPERF: stima della performance
Analisi proposte:
1. Statistiche descrittive
2. Regressione lineare
> >
```{r,eval=TRUE,echo=T,warning=FALSE,message=F,results="asis"}
#-- R CODE
library(pander)
library(car)
library(olsrr)
library(systemfit)
library(het.test)
panderOptions('knitr.auto.asis', FALSE)
#-- White test function
white.test <- function(lmod,data=d){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
#-- funzione per ottenere osservazioni outlier univariate
FIND_EXTREME_OBSERVARION <- function(x,sd_factor=2){
which(x>mean(x)+sd_factor*sd(x) | x<mean(x)-sd_factor*sd(x))
}
#-- import dei dati
ABSOLUTE_PATH <- "C:\\Users\\sbarberis\\Dropbox\\MODELLI STATISTICI"
d <- read.csv(paste0(ABSOLUTE_PATH,"\\F. Esercizi(22) copia\\2.Norm-Col copy(3)\\1.Norm-Col\\cpus.txt"),sep=" ")
#-- vettore di variabili numeriche presenti nei dati
VAR_NUMERIC <- names(d)[3:ncol(d)]
#-- print delle prime 6 righe del dataset
pander(head(d),big.mark=",")
```
> >
## STATISTICHE DESCRITTIVE
Si  presentano innanzitutto le statistiche descrittive.
> >
```{r,fig.width=6,echo=T,message=FALSE,results="asis"}
#-- R CODE
pander(summary(d[,VAR_NUMERIC]),big.mark=",") #-- statistiche descrittive
pander(cor(d[,VAR_NUMERIC]),big.mark=",") #-- matrice di correlazione
plot(d[,VAR_NUMERIC],pch=19,cex=.5) #-- scatter plot multivariato
par(mfrow=c(2,4))
for(i in VAR_NUMERIC){
boxplot(d[,i],main=i,col="lightblue",ylab=i)
}
par(mfrow=c(2,4))
for(i in VAR_NUMERIC){
hist(d[,i],main=i,col="lightblue",xlab=i,freq=F)
}
```
Si costruisce quindi un modello lineare in cui la variabile dipendente "perf" viene regredita rispetto alle variabili esplicative.
## REGRESSIONE
> >
```{r,fig.width=6,echo=T,message=FALSE,results="asis"}
#-- R CODE
mod1 <- lm(perf ~ syct + mmin + mmax + cach + chmin + chmax + estperf, d)
pander(summary(mod1),big.mark=",")
pander(anova(mod1),big.mark=",")
pander(white.test(mod1),big.mark=",")
pander(dwtest(mod1),big.mark=",")
pander(ols_vif_tol(mod1),big.mark=",")
pander(ols_eigen_cindex(mod1),big.mark=",")
```
I modello risulta significativo ma  solo la variabile "estperf" ha associato un parametro che cade nella regione di rifiuto per cui è respinta l’ipotesi nulla di non significatività.
Si esamina quindi la collinearità; come si può notare l’indice di tolleranza è molto piccolo e l’inflation indice è molto grande proprio per "estperf" l’unica variabile significativa, per cui la quota di varianza risulta altresì molto elevata per l’8^ autovalore. "estperf" risulta quindi multicollineare con le altre variabili e viene quindi eliminata. Si effettua una nuova regressione escludendo "estperf".
Si vede come come cambia radicalmente la situazione inerente la significatività delle variabili: "mmin", "mmax", "cach", "chmax" risultano significative. Inoltre nessuna delle variabili è ora collineare.
> >
```{r,fig.width=6,echo=T,message=FALSE,results="asis"}
#-- R CODE
mod1 <- lm(perf ~ syct + mmin + mmax + cach + chmin + chmax, d)
pander(summary(mod1),big.mark=",")
pander(anova(mod1),big.mark=",")
pander(white.test(mod1),big.mark=",")
pander(dwtest(mod1),big.mark=",")
pander(ols_vif_tol(mod1),big.mark=",")
pander(ols_eigen_cindex(mod1),big.mark=",")
```
Si analizza ora la normalità dei residui considerando il modello con le sole variabili significative "mmin", "mmax", "cach", "chmax". Si inizia studiando il valore degli indici di asimmetria e curtosi, la distribuzione dei residui e il box plot.
> >
```{r,fig.width=6,echo=T,message=FALSE,results="asis",warning =FALSE}
#-- R CODE
plot(mod1,which=2,pch=19)
hist(resid(mod1),col="lightblue",freq=F,xlab="Resid",main="")
lines(density(resid(mod1)),col=2,lwd=2)
pander(shapiro.test(resid(mod1)))
pander(ks.test(resid(mod1),"pnorm"))
```
La distribuzione dei residui sembra respingere l’ipotesi di normalità e tutti i test respingono l’ipotesi nulla di normalità.
Sia dal grafico del Q-Q plot che dal confronto dei quantili della distribuzione normale  teorica e osservata si vede la forte discrepanza tra tali distribuzioni. E’ una ulteriore prova della non normalità dei residui.
knitr::opts_chunk$set(echo = TRUE)
d <- read.csv(("/home/pranav/Desktop/Statistical Modelling/Laboratory/datasets/cpus.txt"),sep=" ")
View(d)
#-- vettore di variabili numeriche presenti nei dati
VAR_NUMERIC <- names(d)[3:ncol(d)]
#-- print delle prime 6 righe del dataset
pander(head(d),big.mark=",")
library(pander)
install.packages('pander')
library(pander)
library(car)
library(olsrr)
library(systemfit)
install.packages("systemfit")
library(systemfit)
library(het.test)
panderOptions('knitr.auto.asis', FALSE)
#-- White test function
white.test <- function(lmod,data=d){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
#-- funzione per ottenere osservazioni outlier univariate
FIND_EXTREME_OBSERVARION <- function(x,sd_factor=2){
which(x>mean(x)+sd_factor*sd(x) | x<mean(x)-sd_factor*sd(x))
}
#-- import dei dati
#ABSOLUTE_PATH <- "C:\\Users\\sbarberis\\Dropbox\\MODELLI STATISTICI"
d <- read.csv(("/home/pranav/Desktop/Statistical Modelling/Laboratory/datasets/cpus.txt"),sep=" ")
#-- vettore di variabili numeriche presenti nei dati
VAR_NUMERIC <- names(d)[3:ncol(d)]
#-- print delle prime 6 righe del dataset
pander(head(d),big.mark=",")
pander(summary(mod1),big.mark=",")
#-- R CODE
mod1 <- lm(perf ~ syct + mmin + mmax + cach + chmin + chmax, d)
pander(summary(mod1),big.mark=",")
pander(anova(mod1),big.mark=",")
pander(white.test(mod1),big.mark=",")
pander(dwtest(mod1),big.mark=",")
pander(ols_vif_tol(mod1),big.mark=",")
pander(ols_eigen_cindex(mod1),big.mark=",")
pander(ols_vif_tol(mod1),big.mark=",")
pander(ols_eigen_cindex(mod1),big.mark=",")
plot(mod1,which=2,pch=19)
hist(resid(mod1),col="lightblue",freq=F,xlab="Resid",main="")
lines(density(resid(mod1)),col=2,lwd=2)
pander(shapiro.test(resid(mod1)))
pander(ks.test(resid(mod1),"pnorm"))
library(radiant)
install.packages("radiant")
install.packages("radiant.model")
install.packages(c("radiant.basics", "radiant.data", "radiant.design", "radiant.model", "radiant.multivariate", "radiant"))
library(radiant)
install.packages(c("radiant.model", "radiant.multivariate", "radiant.design", "radiant.data", "radiant.basics"))
library(radiant)
radiant:::radiant()
radiant::radiant()
library(yaml)
library(radiant)
library(radiant.model)
tree = yaml.load_file(input = "./Board_Production.yaml")
result =dtree(yl = tree)
plot(result, final = FALSE)
summary(result, input = FALSE, output = TRUE)
plot(result, final = TRUE)
library(yaml)
library(radiant)
library(radiant.model)
tree = yaml.load_file(input = "./Board_Production.yaml")
setwd("/home/pranav/Desktop/Decision Model/Assignment 1/")
library(yaml)
library(radiant)
library(radiant.model)
tree = yaml.load_file(input = "./Board_Production.yaml")
result =dtree(yl = tree)
plot(result, final = FALSE)
View(tree)
knitr::spin()
getwd()
library(yaml)
library(radiant)
library(radiant.model)
tree = yaml.load_file(input = "./Board_Production.yaml")
result =dtree(yl = tree)
plot(result, final = FALSE)
summary(result, input = FALSE, output = TRUE)
tree
tree$Don't Produce Everything$No Extra Quantity$payoff
;
;
;
;
;
;
;
2+3
tree$`Produce Everything`$`Extra Quantity`$payoff
tree$variables$payoff_not_toghether_not_extra
profitBranch1 <- c(35,-15)
profitBranch2 <- c(10,5)
c(profitBranch1,profitBranch2)
matrix(c(profitBranch1,profitBranch2),nrow = 2)
data.frame(matrix(c(profitBranch1,profitBranch2),nrow = 2))
data.frame(matrix(c(profitBranch1,profitBranch2),nrow = 2))
data.frame("profitBranch1":profitBranch1,"profitBranch2":profitBranch2)
data.frame("profitBranch1"=profitBranch1,"profitBranch2"=profitBranch2)
index <- 1:2
profitBranch1 <- c(35,-15)
profitBranch2 <- c(10,5)
profit <- data.frame("X"=index,"profitBranch1"=profitBranch1,"profitBranch2"=profitBranch2)
profit
CalcBranchCE(profit,R)
utilityFunctionExp <- function(X, R) {
res <- 1- exp(-X/R)
return(res)
}
CertEquivalent = function(EU, R){
CE = -R*ln(1-EU)
return(CE)
}
CalcExpectedUtilityFunction = function(profit, R){
#------Branch 1----------#
UF1 = utilityFunctionExp(profit$profitBranch1, R)
UF1_A = UF1[1]*0.5 + UF1[2]*0.5
#-----Branch 2----------#
UF2 = utilityFunctionExp(profit$profitBranch2, R)
UF2_A = UF2[1]*0.5 + UF2[2]*0.5
#----Return Final Result----#
return(c(UF1_A, UF2_A))
}
CalcBranchCE = function(profit, R){
CE_vett = CertEquivalent(CalcExpectedUtilityFunction(profit, R), R)
return(CE_vett)
}
index <- 1:2
profitBranch1 <- c(35,-15)
profitBranch2 <- c(10,5)
profit <- data.frame("X"=index,"profitBranch1"=profitBranch1,"profitBranch2"=profitBranch2)
R=10
CalcBranchCE(profit,R)
if (CE_Branch2>CE_Branch1)
print("The optimal choise is the second Branch with CE of ") else
print("The optimal choise is the first Branch with CE of ")
CE <- CalcBranchCE(profit,R)
CE_Branch1 <- CE[1]
CE_Branch2 <- CE[2]
if (CE_Branch2>CE_Branch1)
print("The optimal choise is the second Branch with CE of ") else
print("The optimal choise is the first Branch with CE of ")
if (CE_Branch2>CE_Branch1)
print(cat("The optimal choise is the second Branch with CE of ",CE_Branch2)) else
print("The optimal choise is the first Branch with CE of ")
if (CE_Branch2>CE_Branch1)
print(cat("The optimal choise is the second Branch with CE of ",CE_Branch2,"\n")) else
print("The optimal choise is the first Branch with CE of ")
if (CE_Branch2>CE_Branch1)
print(cat("The optimal choise is the second Branch with CE of ",str(CE_Branch2))) else
print("The optimal choise is the first Branch with CE of ")
if (CE_Branch2>CE_Branch1)
print(cat("The optimal choise is the second Branch with CE of ",CE_Branch2)) else
print("The optimal choise is the first Branch with CE of ")
if (CE_Branch2>CE_Branch1)
print("The optimal choise is the second Branch") else
print("The optimal choise is the first Branch")
