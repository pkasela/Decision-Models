---
title: "Assignment 3 - The free position facility location problem"
author: "Pranav Kasela $846965$"
course: "Decision Models"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
  prettydoc::html_pretty:
    theme: architect
    highlight: vignette
    toc: yes
    toc_depth: 5
    df_print: paged
---


## The Problem

In this assignment we will look at the **Facility Location Problem**. 
The problem can be described in general terms as follows: 

We must decide the placement of  a facility that distributes a certain good to a group of consumers that need it.

The placement must to be chosen  in order to **minimize** the total compound distance from the facility and the customers. 

The following assumptions has to be taken into account:

1. The possible location for the facility is unknown, that is the problem is to find the right spot to build it.
2. The facility building costs are fixed and  independent from the position of the building site. 
 


Notice that  in this scenario  there is one possible decision to make: 

- where to build the facility, that is find the position ($\chi$, $\upsilon$) that minimises the compound distance of the facility with respect to all custumers.

### Data

A file with the locations of the consumers can be  found in  the `Data` folder. 

## Distance function

Given the position of the facility $f= (\chi, \upsilon)$ and of a consumer $p_i=(x_i, y_i)$ use the following formula to calculate the distance between them. 

$$
d(f,p_i) = log((\chi-x_i)^2+1) + log((\upsilon-y_i)^2+1)
$$

## The assignment and the solution

1. Formulate the objective function to minimize for the described problem. 

```{r load_dataset,fig.align='center'}
library(ggplot2)

customer_locations <- read.csv("consumer_locations.csv")

ggplot(customer_locations, aes(x=x,y=y)) +
  geom_point() + 
  ggtitle("Customers' Location Distribution") +
  theme(axis.ticks = element_blank(),
        axis.text  = element_blank(),
        panel.grid = element_blank(),
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5))
```

Our Problem in this case is to minimize the function:

\begin{align*}
\text{min}& &f(\bar{P})=\sum_{j=1}^n \text{distance}(\bar{P},\bar{p}_j)
\end{align*}

Where $\bar{P}$ is the location of the facility while $\bar{p}_j$ is the location of the j-th customer, with the distance as defined above.

2. Express in analytical form the gradient for the objective to minimize.

We start calculating the gradient of the function $f(\bar{P})=f(\chi,\upsilon)$:

\[
\nabla = \Big(\sum_i \dfrac{2(\chi-x_i)}{(\chi-x_i)^2 +1},\sum_i \dfrac{2(\upsilon-y_i)}{(\upsilon-y_i)^2 +1}\Big)
\]

3. Implement the `Gradiend Descent method` and solve the problem with it.

```{r gradient descendet}
fn.distance <- function(p1){
  #p1=data.frame(x=x,y=y)
  p2=customer_locations
  sum(log((p1[1]-p2[,"x"])^2+1) + log((p1[2]-p2[,"y"])^2+1))
}

fun.gr <- function(P){
  cl = customer_locations
  c(sum(2*(P[1]-cl[,1])/((P[1]-cl[,1])^2 +1)),
    sum(2*(P[2]-cl[,2])/((P[2]-cl[,2])^2 +1)))
}

# gradient descent function
gradientDescent <- function(par.init, fun.gr, lr, iters){
  par     <- matrix(NA, nrow = iters+1, ncol = 2)
  loss    <- matrix(NA, nrow = iters+1, ncol = 2)
  par[1,] <- par.init
  for(k in 1:iters){
    loss[k,]  <- fun.gr(par[k,])
    par[k+1,] <- par[k,] - lr*loss[k,]
    #stop if the norm of the gradient is smaller the 1e-6 practically 0
    if(sum(loss[k,]^2) < 1e-6)  
      break
  }
  par = par[!is.na(rowSums(par)),]
  distance <- apply(par,1,fn.distance)
  return(list(par       = par,
              dist      = distance,
              opt_point = par[nrow(par),],
              min_dist  = distance[length(distance)]))
}

ris <- gradientDescent(c(300,450),fun.gr,lr = 0.1,iters = 1000)
P   <- ris$par
P_dist <- ris$dist 
plot(1:length(P_dist),P_dist,type = "l",xlab = "Iterations",ylab = "Distance Value")
```


```{r 3D plot of function}
helper.loss <- function(m,c){ #m and c are vectors
dist = matrix(nrow = length(m),ncol = length(c))
for (i in seq(1, length(m), by = 1)) {
 for (j in seq(1, length(c), by = 1)) {
   dist[i,j] = fn.distance(c(m[i], c[j]))   
 } 
}
dist
}

x=seq(0, 1000, 2)
y=seq(0, 1000, 2)
#z <- helper.loss(x,y)

#min(z)
#apply(z,1,which.min)[1]
#apply(z,2,which.min)[1]

#persp(x, y, z, phi = 30, theta = 60,col = "orange",xlab = "x",ylab = "y", zlab = "Distance Function", r=10, d=5)

#contour(x,y,z, xlab="y",ylab="y")
```

```{r Multistart to find the optimal solution}
x <- runif(1000,0,1000)
y <- runif(1000,0,1000)
random.points <- cbind(x,y)

opt_points <- matrix(NA,ncol=2, nrow=length(x))
distances  <- rep(NA,length(x))

ris_temp = list()
for (i in 1:nrow(random.points)){
  strt           <- random.points[i,]
  ris_temp[[i]]       <- gradientDescent(strt,fun.gr,lr = 0.1,iter = 1000)
  opt_points[i,] <- ris_temp[[i]]$opt_point
  distances[i]   <- ris_temp[[i]]$min_dist
}

#ris_temp[[which.min(distances)]]

distances[which.min(distances)]
opt_points[which.min(distances),]
```


4. Solve the problem with a package provided by R (for instance, using the function `optimr` within the package `optimx`). Note that it is not required to use the `gradient descent` algorithm to solve the problem, other algorithms can be used as well.

```{r}

fun.gr <- function(X,y,P){
  cl = cbind(X,y)
  c(sum(2*(P[1]-cl[,1])/((P[1]-cl[,1])^2 +1)),
    sum(2*(P[2]-cl[,2])/((P[2]-cl[,2])^2 +1)))
}

require(curry)

#fun.gr <- curry(curry(fun.gr,customer_locations$x),customer_locations$y)
X=customer_locations$x
y=customer_locations$y

fun1.gr <- curry(curry(fun.gr,X),y)

fun1.gr(c(1,1))

fun.gr <- function(P){
  cl = customer_locations
  c(sum(2*(P[1]-cl[,1])/((P[1]-cl[,1])^2 +1)),
    sum(2*(P[2]-cl[,2])/((P[2]-cl[,2])^2 +1)))
}

fun.gr(c(1,1))
```


```{r,message=F}
require(optimx)
```

```{r optimr solution}
strt   <- c(median(customer_locations[,1]), median(customer_locations[,2]))
ansfgh <- optimr(par = strt, fn = fn.distance, gr = fun.gr,  method="CG")
proptimr(ansfgh) 
```


5. Implement the `Stochastic gradient descent` algorithm with mini-batches and use it to solve the problem. 

```{r Stochastic gradient}
fun.gr <- function(cl,P){
  c(sum(2*(P[1]-cl[,1])/((P[1]-cl[,1])^2 +1)),
    sum(2*(P[2]-cl[,2])/((P[2]-cl[,2])^2 +1)))
}

stoch_gradDescent <- function(dataset,P.init, fun.gr, lr, iters){
  xy <- dataset
  # set matrix theta and set all elements = 0 to start with
  beta <- P.init
    
  # Initialize a matrix to store values of beta for each iteration
  P <- matrix(NA, nrow = iters + 1, ncol = 2)
  P[1,] <- beta
  # set seed value for random sampling
  set.seed(42)
    
  # now iterate using mini batches of randomly sampled  data, updating theta each step
  for (i in 2:iters+1) {
    # randomly sample 5 items from the combined xy data frame
    xysamp <- xy[sample(nrow(xy), floor(nrow(xy)*0.3), 
                                  replace = TRUE), ]
      
    # update beta using mini batches
    beta <- beta - lr  * fun.gr(xysamp, beta)
      
    # save the beta values for iteration i to a matrix for future plotting
    P[i,] <- beta
      
 } # end for loop
 return(P)
}

P = stoch_gradDescent(customer_locations,c(500,500),fun.gr,lr = 0.1,iter = 2000)

P[2001,]

ris <- apply(P,1, fn.distance)
plot(1:2001,ris,type = "l")

fun.gr(cl=customer_locations,c(1,2))
```






